

```
uvicorn research_agent_server:app
```

curl -X POST "https://deepagents.aiagenta2z.com/deep_research/chat" \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"Calculate 1+1 result"}]}'


1. Case 1:
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"Calculate 1+1 result"}]}'

Result

```
{"type": "assistant", "format": "text", "content": "Task Started...", "section": "answer", "message_id": "670d3458-a539-406f-a786-1afc0f0fc201", "content_type": "text/markdown", "template": "streaming_content_type", "task_ids": "", "tab_message_ids": "", "tab_id": ""}
{"type": "assistant", "format": "text", "content": "Calculate 1+1 result", "section": "answer", "message_id": "701be311-37e3-4ee1-9519-6d8e65b47f59", "content_type": "text/markdown", "template": "streaming_content_type", "task_ids": "", "tab_message_ids": "", "tab_id": ""}
{"type": "assistant", "format": "text", "content": "1 + 1 = 2", "section": "answer", "message_id": "lc_run--019c55fe-4ed2-7da3-9e05-0a8758aa10cc-0", "content_type": "text/markdown", "template": "streaming_content_type", "task_ids": "", "tab_message_ids": "", "tab_id": ""}
```


2. Case 2: research context engineering approaches used to build AI agents



curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"research context engineering approaches used to build AI agents"}]}'

```
{"type": "assistant", "format": "text", "content": "Task Started...", "section": "answer", "message_id": "a08dc5a6-b937-42e3-8a7d-5561cd07ba8b", "content_type": "text/markdown", "template": "streaming_content_type", "task_ids": "", "tab_message_ids": "", "tab_id": ""}
{"type": "assistant", "format": "text", "content": "research context engineering approaches used to build AI agents", "section": "answer", "message_id": "2593d7cd-9319-4a9c-a927-ce643e48b5d3", "content_type": "text/markdown", "template": "streaming_content_type", "task_ids": "", "tab_message_ids": "", "tab_id": ""}
{"type": "assistant", "format": "text", "content": "", "section": "answer", "message_id": "lc_run--019c5609-b011-7130-b7bc-47637930ea98-0", "content_type": "text/markdown", "template": "streaming_content_type", "task_ids": "", "tab_message_ids": "", "tab_id": ""}
{"type": "assistant", "format": "text", "content": "Updated todo list to [{'content': 'Save research request to /research_request.md', 'status': 'in_progress'}, {'content': 'Delegate research on context engineering approaches to a sub-agent', 'status': 'pending'}, {'content': 'Synthesize findings and write final report to /final_report.md', 'status': 'pending'}]", "section": "answer", "message_id": "50b24911-5ecd-4388-a268-2a75bf92078a", "content_type": "text/markdown", "template": "streaming_content_type", "task_ids": "", "tab_message_ids": "", "tab_id": ""}
{"type": "assistant", "format": "text", "content": "Updated file /research_request.md", "section": "answer", "message_id": "07667266-86da-4732-860c-e39b9e65e932", "content_type": "text/markdown", "template": "streaming_content_type", "task_ids": "", "tab_message_ids": "", "tab_id": ""}
{"type": "assistant", "format": "text", "content": "", "section": "answer", "message_id": "lc_run--019c5609-ceae-7b20-b5e2-a3cd1308a5a2-0", "content_type": "text/markdown", "template": "streaming_content_type", "task_ids": "", "tab_message_ids": "", "tab_id": ""}
{"type": "assistant", "format": "text", "content": "Updated todo list to [{'content': 'Save research request to /research_request.md', 'status': 'completed'}, {'content': 'Delegate research on context engineering approaches to a sub-agent', 'status': 'in_progress'}, {'content': 'Synthesize findings and write final report to /final_report.md', 'status': 'pending'}]", "section": "answer", "message_id": "22acca0a-c6e3-40ff-ac63-dd61b2302648", "content_type": "text/markdown", "template": "streaming_content_type", "task_ids": "", "tab_message_ids": "", "tab_id": ""}
{"type": "assistant", "format": "text", "content": "Here is a comprehensive report on context engineering approaches for building AI agents, based on current research and emerging practices in 2025-2026.\n\n## Context Engineering Approaches for AI Agents\n\nContext engineering has evolved from simple prompt construction to a sophisticated discipline involving dynamic retrieval, memory management, and agentic reasoning. This report covers the five key areas of focus: RAG architectures, context window management, memory systems, prompt engineering strategies, and emerging frameworks.\n\n### 1. Retrieval-Augmented Generation (RAG) and Its Variations\n\nRAG has matured beyond basic \"vector search + LLM\" pipelines into complex architectures designed to handle ambiguity, multi-step reasoning, and diverse data structures [1].\n\n*   **Vanilla RAG:** The standard \"fetch-then-generate\" pipeline. It is fast and cost-effective but struggles with complex queries and can suffer from low-precision retrieval [1].\n*   **Self-RAG:** Incorporates a \"critic\" loop where the model evaluates its own retrieval quality. If the retrieved documents are irrelevant, the model can decide to re-query or ignore them, significantly reducing hallucinations [1].\n*   **Corrective RAG (CRAG):** Adds a dedicated \"corrector\" step. If retrieved context is deemed low-quality, the system triggers fallback mechanisms like web search or query rewriting before generation [1].\n*   **Graph RAG:** Goes beyond text similarity by retrieving information based on structured relationships in a knowledge graph. This is critical for questions requiring understanding of hierarchy, causality, or complex entity connections [1].\n*   **Hybrid RAG:** Combines dense vector search (good for semantic meaning) with sparse keyword search (BM25, good for exact matches) and other methods to maximize recall across different query types [1].\n*   **Agentic & Multi-Agent RAG:** Instead of a static pipeline, these systems use autonomous agents (e.g., a \"Planner,\" \"Retriever,\" and \"Writer\") to orchestrate the research process. They can break down complex questions into sub-tasks and execute multiple retrieval rounds [1].\n\n### 2. Context Window Management Techniques\n\nAs context windows grow (1M+ tokens), effective management remains critical for cost, latency, and \"lost-in-the-middle\" issues [2].\n\n*   **Sliding Window (FIFO):** Maintains a fixed-size buffer of the most recent messages. It is simple and predictable but suffers from \"catastrophic amnesia\" where key early details are lost [2].\n*   **Summarization (Rolling Summary):** Periodically condenses older conversation history into a concise summary that is retained in the context. This allows for theoretically infinite conversations but causes loss of fine-grained details [2].\n*   **Selective Context (The Bouncer):** Uses a smaller, cheaper model (like a BERT encoder or small LLM) to score and select only the most relevant past messages for the current query, maximizing the signal-to-noise ratio [2].\n*   **Entity Extraction (Knowledge Graph State):** Instead of raw text, an agent extracts and maintains a structured state (e.g., JSON object of user preferences, current tasks). This \"dense\" memory is injected into the system prompt [2].\n*   **Context Caching:** A major efficiency breakthrough where the \"prefix\" of a prompt (e.g., a massive system instruction or codebase) is processed once and cached by the provider. This reduces latency and cost for subsequent requests sharing the same prefix [2].\n*   **Compression:** Techniques like **LLMLingua** use small models to remove non-essential tokens (stopwords, filler) from the prompt, achieving 2-3x compression with minimal accuracy loss [2].\n\n### 3. Memory Systems\n\nAgent memory is now architected similarly to human cognitive systems, distinguishing between immediate working context and long-term storage [3].\n\n*   **Short-Term Memory (STM):** Corresponds to the agent's **Context Window**. It is the \"RAM\" or working memory where active reasoning happens. It is fast but volatile and limited by token counts [3].\n*   **Long-Term Memory (LTM):** The \"Hard Drive\" of the agent, typically implemented using **Vector Databases**. It stores vast amounts of information that can be retrieved when needed via RAG [3].\n*   **Semantic Memory:** Stores general facts and knowledge (e.g., \"The capital of France is Paris\") independent of specific experiences. Often implemented as a knowledge base or vector store [3].\n*   **Episodic Memory:** Stores specific past experiences and events (e.g., \"Last Tuesday, the user asked for help with Python\"). This allows agents to recall past interactions and personalize responses [3].\n*   **Procedural Memory:** The \"muscle memory\" of the agent, representing its skills and how to perform tasks. This is often encoded in the agent's **code, few-shot examples, and system prompts** rather than a database [3].\n*   **MemGPT Architecture:** A notable design where the LLM acts as an operating system, actively managing its own memory hierarchy by deciding when to read/write to long-term storage and what to keep in its limited context window [3].\n\n### 4. Prompt Engineering Strategies for Context\n\nAdvanced prompting has moved from \"art\" to \"engineering,\" with a focus on structured reasoning and optimization [4].\n\n*   **Reasoning Effort:** Replacing simple \"temperature\" tuning, newer models allow developers to control **reasoning effort** (Low/Medium/High), allocating more compute time for hidden \"chain-of-thought\" processing before generating an answer [4].\n*   **Chain-of-Thought (CoT):** Explicitly instructing the model to \"think step-by-step\" or \"explain your reasoning.\" This is essential for complex logic but consumes more tokens [4].\n*   **Chain-of-Symbol (CoS):** An emerging technique for spatial or planning tasks where models reason using symbols (e.g., arrows, grid coordinates) instead of verbose text, which can be more efficient and accurate for certain domains [4].\n*   **Metaprompting:** Using a highly capable \"Reasoning Model\" (e.g., GPT-4o, Claude 3.5 Sonnet) to write optimized system prompts for a smaller, faster \"Production Model.\" This often yields better results than human-written prompts [4].\n*   **DSPy (Declarative Self-improving Language Programs):** A paradigm shift where developers define the \"signature\" (input/output) and a metric, and the framework **compiles** the optimal prompt by automatically iterating and testing variations. This treats prompts as optimized code rather than static strings [4].\n*   **Few-Shot Learning:** Providing 2-5 high-quality examples in the context. This remains one of the most reliable ways to enforce specific formats or behaviors [4].\n\n### 5. Emerging Frameworks for Context Management\n\nNew frameworks are specializing in different aspects of the agent stack, from low-level control to high-level orchestration [5].\n\n*   **LangGraph:** A library for building stateful, multi-agent applications with cyclic graphs. It excels at complex workflows where agents need to loop, retain state, and coordinate, offering fine-grained control over the \"flow\" of context [5].\n*   **DSPy:** Focuses on **programmatic prompt optimization**. It abstracts away the manual string manipulation of prompts, allowing developers to build \"self-improving\" pipelines that optimize their own context usage based on evaluation metrics [5].\n*   **Agno (formerly Phidata):** Positions itself as a production-ready framework with built-in memory abstractions (sessions, storage) and \"Reasoning Tools,\" focusing on developer experience and clean documentation [5].\n*   **InspectAI:** A framework specifically for **evaluating** agent performance. As context management becomes more complex, rigorous evaluation (or \"evals\") is becoming critical to measure the effectiveness of retrieval and memory strategies [5].\n*   **PydanticAI:** A lightweight, type-safe framework that leverages Python's Pydantic library for defining agent schemas and tools, appealing to developers who value code structure and type safety over heavy abstractions [5].\n\n### Sources\n[1] Beyond Vanilla RAG: The 7 Modern RAG Architectures: https://dev.to/naresh_007/beyond-vanilla-rag-the-7-modern-rag-architectures-every-ai-engineer-must-know-4l0c\n[2] Context Window Management - Arun Baby: https://www.arunbaby.com/ai-agents/0012-context-window-management/\n[3] Agent Memory Systems: Short-term, Long-term, and Episodic: https://www.linkedin.com/pulse/day-4-agent-memory-systems-short-term-long-term-episodic-marques-rp3ge\n[4] Prompt Engineering: Advanced Techniques for 2026: https://www.digitalapplied.com/blog/prompt-engineering-advanced-techniques-2026\n[5] Best AI Agent Frameworks in 2025: https://langwatch.ai/blog/best-ai-agent-frameworks-in-2025-comparing-langgraph-dspy-crewai-agno-and-more", "section": "answer", "message_id": "05b2283a-adff-456d-873b-7b8507f1bd04", "content_type": "text/markdown", "template": "streaming_content_type", "task_ids": "", "tab_message_ids": "", "tab_id": ""}
{"type": "assistant", "format": "text", "content": "", "section": "answer", "message_id": "lc_run--019c560b-9287-7242-be29-8e6775a3faa4-0", "content_type": "text/markdown", "template": "streaming_content_type", "task_ids": "", "tab_message_ids": "", "tab_id": ""}
{"type": "assistant", "format": "text", "content": "Updated todo list to [{'content': 'Save research request to /research_request.md', 'status': 'completed'}, {'content': 'Delegate research on context engineering approaches to a sub-agent', 'status': 'completed'}, {'content': 'Synthesize findings and write final report to /final_report.md', 'status': 'in_progress'}]", "section": "answer", "message_id": "5bc3a38f-2cbe-4840-979b-e2bdbb309f82", "content_type": "text/markdown", "template": "streaming_content_type", "task_ids": "", "tab_message_ids": "", "tab_id": ""}
{"type": "assistant", "format": "text", "content": "Updated file /final_report.md", "section": "answer", "message_id": "7ca51d68-05f3-4db5-b0e5-31bd21141811", "content_type": "text/markdown", "template": "streaming_content_type", "task_ids": "", "tab_message_ids": "", "tab_id": ""}
{"type": "assistant", "format": "text", "content": "", "section": "answer", "message_id": "lc_run--019c560b-e033-7e41-9872-46a8b3fc8f33-0", "content_type": "text/markdown", "template": "streaming_content_type", "task_ids": "", "tab_message_ids": "", "tab_id": ""}
{"type": "assistant", "format": "text", "content": "Updated todo list to [{'content': 'Save research request to /research_request.md', 'status': 'completed'}, {'content': 'Delegate research on context engineering approaches to a sub-agent', 'status': 'completed'}, {'content': 'Synthesize findings and write final report to /final_report.md', 'status': 'completed'}]", "section": "answer", "message_id": "00839cf3-4e8d-429c-9845-4a0d4d62ebfa", "content_type": "text/markdown", "template": "streaming_content_type", "task_ids": "", "tab_message_ids": "", "tab_id": ""}
{"type": "assistant", "format": "text", "content": "The research on context engineering approaches for AI agents has been completed. The final report has been saved to `/final_report.md` and covers RAG architectures, context window management, memory systems, prompt engineering, and emerging frameworks.\n\n---\n\n# Context Engineering Approaches for Building AI Agents\n\nContext engineering has evolved from simple prompt construction to a sophisticated discipline involving dynamic retrieval, memory management, and agentic reasoning. This report details the key approaches used to build effective AI agents, focusing on Retrieval-Augmented Generation (RAG), context window management, memory systems, prompt engineering, and emerging frameworks.\n\n## 1. Retrieval-Augmented Generation (RAG) Architectures\n\nRAG has matured beyond basic \"vector search + LLM\" pipelines into complex architectures designed to handle ambiguity, multi-step reasoning, and diverse data structures [1].\n\n*   **Vanilla RAG**: The standard \"fetch-then-generate\" pipeline. It is fast and cost-effective but can struggle with complex queries and low-precision retrieval.\n*   **Self-RAG**: Incorporates a \"critic\" loop where the model evaluates its own retrieval quality. If retrieved documents are irrelevant, the model can decide to re-query or ignore them, significantly reducing hallucinations.\n*   **Corrective RAG (CRAG)**: Adds a dedicated \"corrector\" step. If retrieved context is deemed low-quality, the system triggers fallback mechanisms like web search or query rewriting before generation.\n*   **Graph RAG**: Goes beyond text similarity by retrieving information based on structured relationships in a knowledge graph. This is critical for questions requiring understanding of hierarchy, causality, or complex entity connections.\n*   **Hybrid RAG**: Combines dense vector search (good for semantic meaning) with sparse keyword search (e.g., BM25) to maximize recall across different query types.\n*   **Agentic & Multi-Agent RAG**: Uses autonomous agents (e.g., a \"Planner,\" \"Retriever,\" and \"Writer\") to orchestrate the research process, breaking down complex questions into sub-tasks and executing multiple retrieval rounds.\n\n## 2. Context Window Management\n\nAs context windows grow (1M+ tokens), effective management remains critical for cost, latency, and avoiding \"lost-in-the-middle\" issues [2].\n\n*   **Sliding Window (FIFO)**: Maintains a fixed-size buffer of the most recent messages. It is simple but suffers from \"catastrophic amnesia\" where key early details are lost.\n*   **Summarization (Rolling Summary)**: Periodically condenses older conversation history into a concise summary retained in the context. This allows for long conversations but loses fine-grained details.\n*   **Selective Context**: Uses a smaller, cheaper model to score and select only the most relevant past messages for the current query, maximizing the signal-to-noise ratio.\n*   **Entity Extraction**: Instead of raw text, an agent extracts and maintains a structured state (e.g., JSON object of user preferences, current tasks) which is injected into the system prompt.\n*   **Context Caching**: A major efficiency technique where the \"prefix\" of a prompt (e.g., a massive system instruction or codebase) is processed once and cached by the provider, reducing latency and cost.\n*   **Compression**: Techniques like **LLMLingua** use small models to remove non-essential tokens (stopwords, filler) from the prompt, achieving significant compression with minimal accuracy loss.\n\n## 3. Memory Systems\n\nAgent memory is now architected similarly to human cognitive systems, distinguishing between immediate working context and long-term storage [3].\n\n*   **Short-Term Memory (STM)**: Corresponds to the agent's **Context Window**. It is the \"RAM\" or working memory where active reasoning happens.\n*   **Long-Term Memory (LTM)**: The \"Hard Drive\" of the agent, typically implemented using **Vector Databases**. It stores vast amounts of information retrievable via RAG.\n*   **Semantic Memory**: Stores general facts and knowledge independent of specific experiences (e.g., a knowledge base).\n*   **Episodic Memory**: Stores specific past experiences and events, allowing agents to recall past interactions and personalize responses.\n*   **Procedural Memory**: The \"muscle memory\" of the agent, representing skills and how to perform tasks, often encoded in code, few-shot examples, and system prompts.\n*   **MemGPT Architecture**: A design where the LLM acts as an operating system, actively managing its own memory hierarchy by deciding when to read/write to long-term storage.\n\n## 4. Prompt Engineering Strategies\n\nAdvanced prompting has moved towards structured reasoning and optimization [4].\n\n*   **Reasoning Effort**: Newer models allow developers to control **reasoning effort** (Low/Medium/High), allocating more compute time for hidden \"chain-of-thought\" processing.\n*   **Chain-of-Thought (CoT)**: Explicitly instructing the model to \"think step-by-step,\" essential for complex logic.\n*   **Chain-of-Symbol (CoS)**: Using symbols (e.g., arrows, grid coordinates) for spatial or planning tasks instead of verbose text.\n*   **Metaprompting**: Using a highly capable \"Reasoning Model\" to write optimized system prompts for a smaller, faster \"Production Model.\"\n*   **DSPy**: A framework that treats prompts as optimized code. Developers define inputs/outputs and metrics, and the framework compiles the optimal prompt by automatically iterating and testing variations.\n*   **Few-Shot Learning**: Providing 2-5 high-quality examples in the context to enforce specific formats or behaviors.\n\n## 5. Emerging Frameworks\n\nNew frameworks are specializing in different aspects of the agent stack [5].\n\n*   **LangGraph**: Builds stateful, multi-agent applications with cyclic graphs, offering fine-grained control over the \"flow\" of context.\n*   **DSPy**: Focuses on **programmatic prompt optimization**, abstracting away manual string manipulation.\n*   **Agno (formerly Phidata)**: A production-ready framework with built-in memory abstractions (sessions, storage) and \"Reasoning Tools.\"\n*   **InspectAI**: A framework specifically for **evaluating** agent performance, critical for measuring the effectiveness of retrieval and memory strategies.\n*   **PydanticAI**: A lightweight, type-safe framework utilizing Python's Pydantic library for defining agent schemas and tools.\n\n### Sources\n[1] Beyond Vanilla RAG: The 7 Modern RAG Architectures: https://dev.to/naresh_007/beyond-vanilla-rag-the-7-modern-rag-architectures-every-ai-engineer-must-know-4l0c\n[2] Context Window Management - Arun Baby: https://www.arunbaby.com/ai-agents/0012-context-window-management/\n[3] Agent Memory Systems: Short-term, Long-term, and Episodic: https://www.linkedin.com/pulse/day-4-agent-memory-systems-short-term-long-term-episodic-marques-rp3ge\n[4] Prompt Engineering: Advanced Techniques for 2026: https://www.digitalapplied.com/blog/prompt-engineering-advanced-techniques-2026\n[5] Best AI Agent Frameworks in 2025: https://langwatch.ai/blog/best-ai-agent-frameworks-in-2025-comparing-langgraph-dspy-crewai-agno-and-and-more", "section": "answer", "message_id": "lc_run--019c560b-f56d-7041-a0e6-f4d71eb4a173-0", "content_type": "text/markdown", "template": "streaming_content_type", "task_ids": "", "tab_message_ids": "", "tab_id": ""}
```
